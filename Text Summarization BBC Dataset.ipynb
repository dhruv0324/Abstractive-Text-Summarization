{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5e3647f0-1c7c-4c41-8d9d-349ab5ad7d1c",
   "metadata": {},
   "source": [
    "# Loading the News Article and Reference Summary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d27bccf-9a81-40b5-a916-f15e977b8237",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "38fe7c12-42bc-4aa7-8206-34eda18c1249",
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_dir = r'BBC News Summary\\News Articles\\business'\n",
    "summaries_dir = r'BBC News Summary\\Summaries\\business'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "47b4f9e3-79b5-4b16-b726-9d8e39b207be",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_files = sorted([f for f in os.listdir(articles_dir) if f.endswith('.txt')])\n",
    "summary_files = sorted([f for f in os.listdir(summaries_dir) if f.endswith('.txt')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7d892b44-efde-463c-a2d1-1dde8dd05838",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pairs = list(zip(article_files, summary_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "83db6404-4cba-463a-9c12-21835419a128",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)  \n",
    "selected_pairs = random.sample(file_pairs, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0bb89ed6-1a0e-4b73-b9a4-0730858f3c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed55785-e1f6-43d4-87e1-bd8134b54460",
   "metadata": {},
   "source": [
    "# 1. PEGASUS Model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3b6b0b7c-59b7-4fbb-ad40-a7942b5785e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-large and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
    "pegasus_tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n",
    "pegasus_model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f18a0df3-674e-41c6-92d1-3ceb446e2b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_pegasus(article, max_length=100):\n",
    "    inputs = pegasus_tokenizer.encode(article, return_tensors=\"pt\", truncation=True)\n",
    "    summary_ids = pegasus_model.generate(inputs, max_length=max_length, num_beams=4, early_stopping=True)\n",
    "    return pegasus_tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "219f3fc7-b5eb-41c9-89c4-68655bb8e6a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "pegasus_precision_scores = []\n",
    "pegasus_recall_scores = []\n",
    "pegasus_f1_scores = []\n",
    "\n",
    "# Iterate over the selected pairs\n",
    "for article_file, summary_file in selected_pairs:\n",
    "    article_path = os.path.join(articles_dir, article_file)\n",
    "    summary_path = os.path.join(summaries_dir, summary_file)\n",
    "    \n",
    "    # Read article and reference summary\n",
    "    article = read_file(article_path)\n",
    "    reference_summary = read_file(summary_path)\n",
    "    \n",
    "    # Generate summary using Pegasus\n",
    "    generated_summary = generate_summary_pegasus(article)\n",
    "    \n",
    "    # Evaluate the summary using BERTScore\n",
    "    P, R, F1 = score([generated_summary], [reference_summary], lang='en', verbose=False)\n",
    "    \n",
    "    # Store the individual scores\n",
    "    pegasus_precision_scores.append(P.item())\n",
    "    pegasus_recall_scores.append(R.item())\n",
    "    pegasus_f1_scores.append(F1.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "b6627105-315d-4f60-9d18-1ff2b18361a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Precision    Recall  F1 Score\n",
      "0    0.957410  0.862893  0.907697\n",
      "1    0.933025  0.844105  0.886340\n",
      "2    0.959055  0.885500  0.920811\n",
      "3    0.940711  0.861259  0.899233\n",
      "4    0.956425  0.878585  0.915854\n",
      "5    0.908054  0.850337  0.878248\n",
      "6    0.896424  0.837705  0.866070\n",
      "7    0.900026  0.846090  0.872225\n",
      "8    0.849548  0.850108  0.849828\n",
      "9    0.880596  0.840135  0.859890\n",
      "10   0.951864  0.871208  0.909752\n",
      "11   0.926327  0.896395  0.911115\n",
      "12   0.954179  0.861950  0.905722\n",
      "13   0.951534  0.838982  0.891720\n",
      "14   0.948883  0.860580  0.902577\n",
      "15   0.908829  0.846684  0.876656\n",
      "16   0.979966  0.893668  0.934830\n",
      "17   0.965993  0.861489  0.910753\n",
      "18   0.964793  0.906781  0.934888\n",
      "19   0.944252  0.888565  0.915563\n"
     ]
    }
   ],
   "source": [
    "pegasus_business = pd.DataFrame({\n",
    "    'Precision': pegasus_precision_scores,\n",
    "    'Recall': pegasus_recall_scores,\n",
    "    'F1 Score': pegasus_f1_scores\n",
    "})\n",
    "\n",
    "print(pegasus_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "981ffec5-379e-4224-bbf5-5acbc2703634",
   "metadata": {},
   "outputs": [],
   "source": [
    "pegasus_business.to_excel('pegasus_business.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02053f49-8d17-4dc9-9a44-4854de48c11d",
   "metadata": {},
   "source": [
    "# 2. BART Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d234398a-0363-4517-aa44-3a3435a56572",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large\")\n",
    "bart_model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ca5c0697-026a-456d-8449-6cc93993c991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_bart(article, max_length=100):\n",
    "    inputs = bart_tokenizer.encode(article, return_tensors=\"pt\", truncation=True)\n",
    "    summary_ids = bart_model.generate(inputs, max_length=max_length, num_beams=4, early_stopping=True)\n",
    "    return bart_tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f7ebc482-2218-4a1d-94a5-6fe94d375c0d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "bart_precision_scores = []\n",
    "bart_recall_scores = []\n",
    "bart_f1_scores = []\n",
    "\n",
    "# Iterate over the selected pairs\n",
    "for article_file, summary_file in selected_pairs:\n",
    "    article_path = os.path.join(articles_dir, article_file)\n",
    "    summary_path = os.path.join(summaries_dir, summary_file)\n",
    "    \n",
    "    # Read article and reference summary\n",
    "    article = read_file(article_path)\n",
    "    reference_summary = read_file(summary_path)\n",
    "    \n",
    "    # Generate summary using BART\n",
    "    generated_summary = generate_summary_bart(article)\n",
    "    \n",
    "    # Evaluate the summary using BERTScore\n",
    "    P, R, F1 = score([generated_summary], [reference_summary], lang='en', verbose=False)\n",
    "    \n",
    "    # Store the individual scores\n",
    "    bart_precision_scores.append(P.item())\n",
    "    bart_recall_scores.append(R.item())\n",
    "    bart_f1_scores.append(F1.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "229aa119-835c-491b-95f7-3e599a6dd3ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Precision    Recall  F1 Score\n",
      "0    0.864749  0.875710  0.870195\n",
      "1    0.914950  0.884959  0.899704\n",
      "2    0.888840  0.868622  0.878615\n",
      "3    0.901507  0.899700  0.900603\n",
      "4    0.894263  0.879567  0.886854\n",
      "5    0.900553  0.861527  0.880608\n",
      "6    0.852954  0.859692  0.856310\n",
      "7    0.861058  0.844869  0.852887\n",
      "8    0.863347  0.885605  0.874334\n",
      "9    0.919045  0.890211  0.904398\n",
      "10   0.914039  0.885249  0.899414\n",
      "11   0.897617  0.875244  0.886290\n",
      "12   0.913847  0.902328  0.908051\n",
      "13   0.897739  0.863924  0.880507\n",
      "14   0.688211  0.805876  0.742410\n",
      "15   0.840859  0.817367  0.828946\n",
      "16   0.834498  0.845945  0.840183\n",
      "17   0.883742  0.868690  0.876151\n",
      "18   0.680873  0.796287  0.734071\n",
      "19   0.921112  0.899490  0.910172\n"
     ]
    }
   ],
   "source": [
    "bart_business = pd.DataFrame({\n",
    "    'Precision': bart_precision_scores,\n",
    "    'Recall': bart_recall_scores,\n",
    "    'F1 Score': bart_f1_scores\n",
    "})\n",
    "\n",
    "print(bart_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a566046e-a8c1-4bf1-be28-7ec07bc43619",
   "metadata": {},
   "outputs": [],
   "source": [
    "bart_business.to_excel('bart_business.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57046457-b8d1-4cd4-9bee-86e118bd812d",
   "metadata": {},
   "source": [
    "# 3. T5 Model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f2483963-80dc-4f24-99f9-583ba8a006cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "t5_tokenizer = T5Tokenizer.from_pretrained(\"t5-large\")\n",
    "t5_model = T5ForConditionalGeneration.from_pretrained(\"t5-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "95be3d68-fc6e-46a1-9341-c78171bc579c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_t5(article, max_length=100):\n",
    "    inputs = t5_tokenizer.encode(\"summarize: \" + article, return_tensors=\"pt\", truncation=True)\n",
    "    summary_ids = t5_model.generate(inputs, max_length=max_length, num_beams=4, early_stopping=True)\n",
    "    return t5_tokenizer.decode(summary_ids[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "d456529d-e2f6-438d-b960-97849e107c27",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "t5_precision_scores = []\n",
    "t5_recall_scores = []\n",
    "t5_f1_scores = []\n",
    "\n",
    "# Iterate over the selected pairs\n",
    "for article_file, summary_file in selected_pairs:\n",
    "    article_path = os.path.join(articles_dir, article_file)\n",
    "    summary_path = os.path.join(summaries_dir, summary_file)\n",
    "    \n",
    "    # Read article and reference summary\n",
    "    article = read_file(article_path)\n",
    "    reference_summary = read_file(summary_path)\n",
    "    \n",
    "    # Generate summary using BART\n",
    "    generated_summary = generate_summary_bart(article)\n",
    "    \n",
    "    # Evaluate the summary using BERTScore\n",
    "    P, R, F1 = score([generated_summary], [reference_summary], lang='en', verbose=False)\n",
    "    \n",
    "    # Store the individual scores\n",
    "    t5_precision_scores.append(P.item())\n",
    "    t5_recall_scores.append(R.item())\n",
    "    t5_f1_scores.append(F1.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "922861fe-d546-424d-9fb9-17eee8ff1b97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Precision    Recall  F1 Score\n",
      "0    0.864749  0.875710  0.870195\n",
      "1    0.914950  0.884959  0.899704\n",
      "2    0.888840  0.868622  0.878615\n",
      "3    0.901507  0.899700  0.900603\n",
      "4    0.894263  0.879567  0.886854\n",
      "5    0.900553  0.861527  0.880608\n",
      "6    0.852954  0.859692  0.856310\n",
      "7    0.861058  0.844869  0.852887\n",
      "8    0.863347  0.885605  0.874334\n",
      "9    0.919045  0.890211  0.904398\n",
      "10   0.914039  0.885249  0.899414\n",
      "11   0.897617  0.875244  0.886290\n",
      "12   0.913847  0.902328  0.908051\n",
      "13   0.897739  0.863924  0.880507\n",
      "14   0.688211  0.805876  0.742410\n",
      "15   0.840859  0.817367  0.828946\n",
      "16   0.834498  0.845945  0.840183\n",
      "17   0.883742  0.868690  0.876151\n",
      "18   0.680873  0.796287  0.734071\n",
      "19   0.921112  0.899490  0.910172\n"
     ]
    }
   ],
   "source": [
    "t5_business = pd.DataFrame({\n",
    "    'Precision': t5_precision_scores,\n",
    "    'Recall': t5_recall_scores,\n",
    "    'F1 Score': t5_f1_scores\n",
    "})\n",
    "\n",
    "print(t5_business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "51dce5da-78d5-4e39-8613-a9a5762c09ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "t5_business.to_excel('t5_business.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1929621a-30ca-424d-bdbd-28e922b8ea3c",
   "metadata": {},
   "source": [
    "# Sample News Article and Reference Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "94c82756-4c18-42d0-8ce2-5fd2e692cf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft releases bumper patches\n",
      "\n",
      "Microsoft has warned PC users to update their systems with the latest security fixes for flaws in Windows programs.\n",
      "\n",
      "In its monthly security bulletin, it flagged up eight \"critical\" security holes which could leave PCs open to attack if left unpatched. The number of holes considered \"critical\" is more than usual. They affect Windows programs, including Internet Explorer (IE), media player and instant messaging. Four other important fixes were also released. These were considered to be less critical, however. If not updated, either automatically or manually, PC users running the programs could be vulnerable to viruses or other malicious attacks designed to exploit the holes. Many of the flaws could be used by virus writers to take over computers remotely, install programs, change, and delete or see data.\n",
      "\n",
      "One of the critical patches Microsoft has made available is an important one that fixes some IE flaws. Stephen Toulouse, a Microsoft security manager, said the flaws were known about, and although the firm had not seen any attacks exploiting the flaw, he did not rule them out. Often, when a critical flaw is announced, spates of viruses follow because home users and businesses leave the flaw unpatched. A further patch fixes a hole in Media Player, Windows Messenger and MSN Messenger which an attacker could use to take control of unprotected machines through .png files. Microsoft announces any vulnerabilities in its software every month. The most important ones are those which are classed as \"critical\". Its latest releases came the week that the company announced it was to buy security software maker Sybari Software as part of Microsoft's plans to make its own security programs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = r'BBC News Summary\\News Articles\\tech\\007.txt' \n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4218b08b-9197-4eea-b753-4f196aff2836",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft has warned PC users to update their systems with the latest security fixes for flaws in Windows programs.One of the critical patches Microsoft has made available is an important one that fixes some IE flaws.In its monthly security bulletin, it flagged up eight \"critical\" security holes which could leave PCs open to attack if left unpatched.Often, when a critical flaw is announced, spates of viruses follow because home users and businesses leave the flaw unpatched.The most important ones are those which are classed as \"critical\".The number of holes considered \"critical\" is more than usual.\n"
     ]
    }
   ],
   "source": [
    "file_path = r'BBC News Summary\\Summaries\\tech\\007.txt' \n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    reference_summary = file.read()\n",
    "\n",
    "print(reference_summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6688f3-939c-49d5-9c87-631ffe0aa624",
   "metadata": {},
   "source": [
    "# Model Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776209ae-7c39-4e6a-a5c5-1aae917e3354",
   "metadata": {},
   "source": [
    "## PEGASUS:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a7ad689a-4200-4639-aafd-eec1245aa10d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft releases bumper patches Microsoft has warned PC users to update their systems with the latest security fixes for flaws in Windows programs. Its latest releases came the week that the company announced it was to buy security software maker Sybari Software as part of Microsoft's plans to make its own security programs.\n"
     ]
    }
   ],
   "source": [
    "pegasus_summary = generate_summary_pegasus(text)\n",
    "print(pegasus_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b1ac6d-6f10-4324-82cc-72cb77692b9c",
   "metadata": {},
   "source": [
    "## BART:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f129292d-17cd-4c57-acaf-47b0be595ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft releases bumper patches for 'critical' security holes in Windows programsImage copyright Getty Images Image caption Microsoft has warned PC users to update their systems with the latest security fixes for flaws in Windows productsMicrosoft has warned PCs running its Windows operating system that they could be vulnerable to viruses or other malicious attacks designed to exploit the holes.In its monthly security bulletin, it flagged up eight \"critical\" security holes which could leave PCs open to attack if left unpatched. They affect Windows programs,\n"
     ]
    }
   ],
   "source": [
    "bart_summary = generate_summary_bart(text)\n",
    "print(bart_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb177d99-80a4-4832-aa32-ccdb8f95009f",
   "metadata": {},
   "source": [
    "## T5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "295972a1-04e0-457f-9b33-f4a5b9901c21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft has warned PC users to update their systems with the latest security fixes for flaws in windows programs. it flagged up eight \"critical\" security holes which could leave PCs open to attack if left unpatched. many of the flaws could be used by virus writers to take over computers remotely, install programs, change, and delete or see data.\n"
     ]
    }
   ],
   "source": [
    "t5_summary = generate_summary_t5(text)\n",
    "print(t5_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111fe2ae-7505-422a-8b07-4dfb0b295529",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
